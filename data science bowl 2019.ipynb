{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science Bowl 2019\n\nThis script contains the code used to generate my 204th place solution to the 2019 data science bowl.  Some sections of the code are taken from public notebooks - this is acknowledged for these cell blocks but no links are provided since I am writing this some time after the competition ended.  The purpose of this write-up is as a summary of the lessons learned throughout this competition. \n\n## Objective\n\nThe objective is to classify the performance of users of the PBS kids app using information about the user's activities.  Each user has a unique identification code.  The performance is measured by the number of attempts a user took before successfully completed an assessment.  There are four classes:\n\n*  3 - The user was successful on their first attempt.\n*  2 - The user was successful on their second attempt.\n*  1 - The user successfully completed the assessment task on any subsequent attempt.\n*  0 - The user attempted but did not successfully complete the assessment.\n\nThe predictions are scored using the quadratic weighted kappa metric, which is a measurement of agreement between two outcomes.  This metric penalises predictions that are further from the actual class - this is important for this competition since each category is not incomparable to the others.  Intuitively, for a ground truth class of 3, a model predicting 2 should be better than a model predicting 0.\n\n## Model\n\nThe model chosen for this task is a lightgbm gradient boosted decision tree.  This model was chosen for it's predictive accuracy and fast training.  5 group folds were used for cross-validation, with the groups being split by user identification code.  This means that each validation set contains identification codes not present in the training set.  This validation method was chosen as it emulates the test data, which contains many user identification codes not present in the training set.  \n\nThough this is a classification task, a regression model was used with cutoff points used to generate class predictions.  The cutoff points are chosen uses a minimiser imported from the scipy module.  The prediction process is best explained using an example - say the model predicts 1.4 for the user's performance on some assessment.  The cutoff points chosen by the minimiser for the class with the label 1 are (0.7, 1.5).  The prediction lies in this interval, so the class predicted by the model is 1.  This prediction method had a better performance as measured by the group-fold cross-validation and the public leaderboard.  The probably reason for this increase in performace is because the quadratic weighted kappa metric does not penalise wrong predictions equally for each class - the classification objective does not capture this effect.\n\n## Features\n\nThe contents of the data are explained on the competition page.  Hundreds of features were extracted for use in the model prior to any feature selection.  Typical features extracted were past performance on assessments, assessment type, games played in the app and summary statistics for the event codes.  The meaning of particular event codes are explained in the 'specs' file - the more useful codes were usually the codes indicating a game has started or finished, or a particular level was reached inside an activity.  The summary statistics were usually either the sum or the mean number for a particular event code.\n\nThe most consistently useful feature was a user's past performance on any type of assessment.  Almost every other feature had a very small impact on the predictive accuracy.  This was a clear challenge for the task - there are hundreds of available features and almost all of them had very little impact on the performance as measured by the quadratic weighted kappa.  To address this, a sort of backward stepwise selection was used - the full model was scored using the cross-validation method described above.  For each step, a random selection of 10 percent of the total features were removed and a new model was trained and scored.  This method is intractable for every possible subset of features, so a fixed number of trials was used instead.  After these trials are completed, the best model as measured by the cross-validation method is chosen, and the process is repeated.  This is repeated for a fixed number of steps, resulting in a model with far fewer features used.  The first model used 898 features - a typical final model after this process would have about 500 features.  The number of steps was chosen based on the time taken to train each model - a typical model takes about 80 seconds to train including the group-fold validation, and the entire script should run in less then 9 hours. The total number of models trained should then be around 400 or less.  \n\n## Ensembling\n\nFor this section, a model actually refers to the collection of 5 models trained using the group-fold validation method described in the model summary.\n\nA holdout set was created for use in ensemble scoring from the test set.  The predictions are evaluated in this competition on the final assessment of each user in the test set.  I used the assessments prior to the final assessment for validation.\n\nFor each model trained, the score, the features used and the predictions on the holdout set are stored in a dictionary. The best 6 scoring models are then chosen as candidates for use in a simple linear regression model which predicts the class using a combination of only three of these model's predictions.  \n\nThe best subset of three models is chosen using the holdout predictions by first optimising the regression coefficients, then optimising the cutoff points used to transform regression predictions into class predictions.  Each subset is then scored using the combined out-of-fold predictions.  \n\nThe three best models are then used to generate test predictions which are combined using the linear regression coefficients and transformed to class predictions using the cutoff points optimised based on the quadratic weighted kappa mettric.\n\n## Retrospective\n\nThis is my second competition on kaggle, the first being the ASHRAE Great Energy Predictor III.  The objective in the ASHRAE competition was to predict the meter readings for energy, steam, etc.  In the first competition, there were significant data leaks which were used by competitors as ensembling tools allowing very good scores on the public leaderboard.  The private dataset was not similar to the public dataset, and competitors such as myself who sought to use the leaks without a stable local validation method were punished.  I personally went from 240th on the public leaderboard to 799th on the private leaderboard.  I learned two main lessons from this competition - firstly, it is pointless to make modelling decisions such as feature selection or parameter tuning without a method that allows you to consistently determine whether an improvement has actually been made.  \n\nI feel my attempts to address this issue in this competition were fruitful - my private leadboard score was correlated fairly strongly with both my local validation and the public leaderboard.  A particular problem with cross-validation for this competition was that the number of users who had no prior history on other assessments was much higher in the test set.  In earlier kernels I was using a sort of pseudo-bootstrapping technique to address this - samples were selected from the training set with replacement, with the class for each sample selected with probability corresponding to it's proportional appearance in the test set.  For example - about 40% of the test set samples had no prior assessment history.  The sample taken from the training set was chosen with a 40% probability of having no prior assessment history.  This process was repeated to generate many validation sets, and the scores were estimated using out-of-fold predictions.  This method had a higher correlation with the public leaderboard than the standard group-fold validation method.  The method was dropped in favour of a less stable, yet faster training method using only group-k-fold.  The justification was that the group-fold validation was adequate, and the improvement in training time was desirable despite a less stable cross-validation.\n\nThe second lesson learned from the ASHRAE competition was that cleaning a dataset is just as important as model tuning or ensembling.  The ASHRAE dataset was characterised by long streaks of zero readings for the various energy meters - all the winning models took significant steps to remove the entries corresponding to probable faulty meters.  This particular lesson was not as relevant for this competition since there was little evidence of bad data samples.\n\nAs for this competition - there were several problems in my approach which were evident upon looking at the winning submissions.  The most important, in my opinion, was my crude method of feature selection.  This was a particular issue for me in this competition - removing features that were measured to be non-influential by standard metrics available in lightgbm such as the gain importance and split importance did not lead to better validation scores.  My attempt to address this was to simply minimise the validation score by removing random subsets of up to 80 features at a time - frequently removing significant features.  I learned of permutation importance after reading the winning solution.  \n\nPermutation importance works by creating a distribution of feature importances by randomly shuffling the class labels and recording the feature importances for each variable.  Repeating this allows an estimate of the mean importance of a particular variable when the labels are shuffled.  The mean of a sample is approximately normally distributed by the central limit theorem - this allows for a confidence interval to be assigned to the importance of each variable.  The importances of each variable when the labels are not shuffled can then be compared with these confidence intervals using p-values and standard hypothesis testing - if a variable has an importance which is considered too unlikely using some predetermined signficance level, it is probably useful for predicting the response variable.  This method was particularly effective here for several reasons; the training time is low enough to generate a reasonable null distribution, there are large numbers of variables which are highly correlated, most of which are not significant for predicting the correct class.  More information on this method is available here: \n\nhttps://academic.oup.com/bioinformatics/article/26/10/1340/193348\n\nThe second most important issue for me in this competition was my lack of version control.  At the end of the competition I have 147 versions of this code.  A frequent problem arises where I will want to reuse code I wrote 20 versions ago, but don't remember which version it was in.  I could also improve by writing down the ideas that I'm working on rather than relying on memory.\n\nMy focus going forward will be to improve version control, and to be more aware of state of the art methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shortened the runtime since I only want the kernel for presentation purposes.  Set to true to run the original version.\n\nrun_long_version = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport scipy as sp\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(28)\n\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nfrom typing import Any\nfrom numba import jit\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom itertools import product\nimport copy\nimport time\nimport json\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in the csv files.  Removed the entries corresponding to the users who were active in the app but did not record and assessment results - they won't be very useful for the task.\n\nroot = '/kaggle/input/data-science-bowl-2019/'\n\ntrain = pd.read_csv(root + 'train.csv')\ntrain_labels = pd.read_csv(root + 'train_labels.csv')\nspecs = pd.read_csv(root + 'specs.csv')\ntest = pd.read_csv(root + 'test.csv')\n\ntrain = train[train.installation_id.isin(train_labels.installation_id.unique())]\nkeep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dictionary recording the number of seconds for each clip.  Used in features recording time spent watching clips.\n\nclip_time = {'Welcome to Lost Lagoon!':19,'Tree Top City - Level 1':17,'Ordering Spheres':61, 'Costume Box':61,\n        '12 Monkeys':109,'Tree Top City - Level 2':25, 'Pirate\\'s Tale':80, 'Treasure Map':156,'Tree Top City - Level 3':26,\n        'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18,\n        'Balancing Act':72, 'Lifting Heavy Things':118,'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19,\n        'Heavy, Heavier, Heaviest':61}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dictionary of codes which have the description in each corresponding string.\n\ndescr = \"how much time elapsed while the game was presenting feedback?\"\nfeedback_ids = specs.loc[specs['info'].str.contains(descr)]['event_id'].values\n\ndescr2 = \"how much time elapsed while the game was presenting instruction?\"\ninstruction_ids = specs.loc[specs['info'].str.contains(descr2)]['event_id'].values\n\ndescr3 = \"tutorial\"\ntutorial_ids = specs.loc[specs['info'].str.contains(descr3)]['event_id'].values\n\nmisses_ids = specs.loc[specs['args'].str.contains('misses')]['event_id'].values\n\nid_list_dict = {'feedback_ids':feedback_ids, 'instruction_ids':instruction_ids, 'tutorial_ids':tutorial_ids, 'misses_ids':misses_ids}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Borrowed entirely from a public notebook.  \n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = (list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = (list(set(train['title'].unique()).union(set(test['title'].unique()))))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = (list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n    list_of_event_id = (list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = (list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    train['hour'] = train['timestamp'].dt.hour\n    test['hour'] = test['timestamp'].dt.hour\n    train['weekday'] = train['timestamp'].dt.weekday\n    test['weekday'] = test['timestamp'].dt.weekday\n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mostly taken from a public notebook.  Added time duration features for different classes of activities.  Added features for the JSON data using the \n# dictionaries created from the specs file in the cell above.\n\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0 \n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    clip_durations = []\n    Activity_durations = []\n    Game_durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    \n    game_event_code_count: Dict[str, int] = { str(ev) + '_g': 0 for ev in list_of_event_code}\n    Activity_event_code_count: Dict[str, int] = {str(ev) + '_A': 0 for ev in list_of_event_code}    \n    Activity_sum_event_count = 0\n    game_sum_event_count = 0\n    get_event_data_dict = {'feedback_ids':0, 'instruction_ids':0, 'tutorial_ids':0, 'misses_ids':0}\n #   event_id_duration_mean = {f'dur_mean_{idx}': 0 for idx in list_of_event_id}\n #   event_id_round = {f'round_{idx}': 0 for idx in list_of_event_id}\n #   event_id_level = {f'level_{idx}': 0 for idx in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    # itarates through each session of one instalation_id\n    sessions_count = 0\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]     \n        \n        if session_type == 'Clip':\n            clip_durations.append((clip_time[activities_labels[session_title]]))\n        \n        if session_type == 'Activity':\n            Activity_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = str(k) + '_A'\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            Activity_event_code_count = update_counters(Activity_event_code_count, \"event_code\")\n        \n        if session_type == 'Game':\n            Game_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = str(k) + '_g'\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            game_event_code_count = update_counters(game_event_code_count, \"event_code\")\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            features.update(get_event_data_dict)\n            features.update()\n            features['installation_session_count'] = sessions_count\n            features['hour'] = session['hour'].iloc[-1]\n            features['weekday'] = session['weekday'].iloc[-1]\n            variety_features = [('var_event_code', event_code_count),\n                              ('var_event_id', event_id_count),\n                               ('var_title', title_count),\n                               ('var_title_event_code', title_event_code_count)]\n            \n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            features['game_sum_event_count'] = game_sum_event_count\n            features['Activity_sum_event_count'] = game_sum_event_count\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                \n            if clip_durations == []:\n                features['Clip_duration_mean'] = 0\n                features['Clip_duration_std'] = 0\n            else:\n                features['Clip_duration_mean'] = np.mean(clip_durations)\n                features['Clip_duration_std'] = np.std(clip_durations)\n                \n            if Activity_durations == []:\n                features['Activity_duration_mean'] = 0\n                features['Activity_duration_std'] = 0\n            else:\n                features['Activity_duration_mean'] = np.mean(Activity_durations)\n                features['Activity_duration_std'] = np.std(Activity_durations)\n                \n            if Game_durations == []:\n                features['Game_duration_mean'] = 0\n                features['Game_duration_std'] = 0\n            else:\n                features['Game_duration_mean'] = np.mean(Game_durations)\n                features['Game_duration_std'] = np.std(Game_durations)\n                \n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        sessions_count += 1\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n        \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        \n        # json data\n        for id_list in get_event_data_dict.keys():\n            if id_list == 'misses':\n                just_ids = session.loc[session['event_id'].isin(id_list_dict[f'{id_list}'])]['event_data'].apply(lambda x:json.loads(x).get('misses', None)).sum()\n            else:\n                just_ids = session.loc[session['event_id'].isin(id_list_dict[f'{id_list}'])]['event_data'].apply(lambda x:json.loads(x).get('duration', None)).mean()\n            if just_ids == just_ids:\n                get_event_data_dict[f'{id_list}'] += just_ids\n                get_event_data_dict[f'{id_list}'] /= 2\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            \n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1], all_assessments[:-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Call the utility function, from public notebook.\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mostly from public notebook, modified slightly to include the dataframe \"reduce_test_tr\".  The original cell \n# discarded assessments in the test set prior to the final assessment - the only assessment used for evaluation in this competition.\n# I chose to include assessments prior to the final assessment for validation purposes.\n\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    compiled_test_tr = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 3614):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data, test_data_tr = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n        compiled_test_tr += (test_data_tr)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    reduce_test_tr = pd.DataFrame(compiled_test_tr)    \n    categoricals = ['session_title']\n    return reduce_train, reduce_test, reduce_test_tr, categoricals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates the datasets.\n\nreduce_train, reduce_test, reduce_test_tr, categoricals = get_train_and_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From public notebook.  Only change is to include the reduce_test_tr dataframe.\n\ndef preprocess(reduce_train, reduce_test, reduce_test_tr):\n    for df in [reduce_train, reduce_test, reduce_test_tr]:\n        df['installation_session_clip_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_session_game_count'] = df.groupby(['installation_id'])['Game'].transform('count')\n        df['installation_session_act_count'] = df.groupby(['installation_id'])['Activity'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['accuracy_group', 'game_session']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, reduce_test_tr, features\n# call feature engineering function\nreduce_train, reduce_test, reduce_test_tr, features = preprocess(reduce_train, reduce_test, reduce_test_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mostly from public notebook.  Fixes bugs arising from using JSON strings for lightgbm feature names.\n\ny = reduce_train['accuracy_group']\ny_t = reduce_test_tr['accuracy_group']\nreduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\nreduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]\nreduce_test_tr.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test_tr.columns]\nfeatures = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions used to calculate the cohen kappa metric, generate class predictions from regression predictions, optimise cutoff points.\n# Some of these functions are from public kernels.  Others are from previous versions where I used the validation technique that attempted to emulate \n# the number of prior assessments.  \n\nfrom functools import partial\nfrom sklearn.base import BaseEstimator, TransformerMixin\n@jit\n\ndef use_coefs(oof, coefs, train_preds=True):\n    oof[oof <= coefs[0]] = 0\n    oof[np.where(np.logical_and(oof > coefs[0], oof <= coefs[1]))] = 1\n    oof[np.where(np.logical_and(oof > coefs[1], oof <= coefs[2]))] = 2\n    oof[oof > coefs[2]] = 3\n    if train_preds == True:\n        return qwk(y, oof)\n    else:\n        return oof\n    \ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype='int')\n    a2 = np.asarray(a2, dtype='int')\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\ndef obj_func(guess, y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    s = y_pred.copy()\n    s[s <= guess[0]] = 0\n    s[np.where(np.logical_and(s > guess[0], s <= guess[1]))] = 1\n    s[np.where(np.logical_and(s > guess[1], s <= guess[2]))] = 2\n    s[s > guess[2]] = 3\n    score = -qwk(s, y_true)\n    \n    return score\n\ndef kappa_funcc(guess, y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    labels = y_true.get_label()\n    y_pred[y_pred <= guess[0]] = 0\n    y_pred[np.where(np.logical_and(y_pred > guess[0], y_pred <= guess[1]))] = 1\n    y_pred[np.where(np.logical_and(y_pred > guess[1], y_pred <= guess[2]))] = 2\n    y_pred[y_pred > guess[2]] = 3\n    \n    return 'cappa', qwk(y_pred, labels), True\n\ndef obs(x0, valid_ids, oof, y):\n    scores = []\n    for h in range(iters):\n        scores.append(obj_func(coefs, oof[valid_ids[:,h]], y[valid_ids[:,h]]))\n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create list of features.  Code removes duplicates created when fixing the broken JSON feature names so the dataset is usable in lightgbm. \n\nfeat = [x for x in features if x not in ['installation_id']]\ntarget = ['accuracy_group']\ncategoricals=[]\nduplicates = [k for k,v in Counter(feat).items() if v>1]\nfeat = list(set(feat) - set(duplicates))\n[k for k,v in Counter(feat).items() if v>1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that trains a 5-group-fold lightgbm model.  Accepts parameter dictionary and feature list as parameters.\n# Returns cohen kappa CV score, all 5 models, list of predictions on the reduce_test_tr test set.\n\ndef train_lgb(params, feat):\n    n_fold = 5\n    folds = GroupKFold(n_splits=n_fold)\n    oof = np.zeros(y.shape[0])\n    models = []\n    coefs = [1.09122991, 1.75101234, 2.24616994]\n    print(len(feat))\n    for fold, (tr_idx, v_idx) in enumerate(folds.split(reduce_train, y, reduce_train['installation_id'])):\n        print(f'Fold: {fold}\\n{\"*\"*50}')\n        X_train, y_train = reduce_train.iloc[tr_idx], y.iloc[tr_idx]\n        X_valid, y_valid = reduce_train.iloc[v_idx], y.iloc[v_idx]\n\n        lgb_train = lgb.Dataset(X_train[feat], y_train, categorical_feature=categoricals)\n        lgb_valid = lgb.Dataset(X_valid[feat], y_valid, categorical_feature=categoricals)\n\n        gbm_regress = lgb.train(params, lgb_train, verbose_eval=50, valid_sets=[lgb_train, lgb_valid], num_boost_round=5000,\n                                early_stopping_rounds=50, categorical_feature=categoricals)\n        oof[v_idx] = gbm_regress.predict(X_valid[feat], num_iteration=gbm_regress.best_iteration, random_state=28)\n        models.append(gbm_regress)    \n    y_pred = sum([model.predict(reduce_test_tr[feat], num_iteration=model.best_iteration) for model in models])/n_fold\n    y_true = y_t\n    opt2 = sp.optimize.minimize(fun=obj_func, x0=np.array([1.22232214, 1.73925866, 2.22506454]), args=(y_pred, y_true), method='nelder-mead')\n    score = opt2.fun\n    print(score)\n    return score, models, y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These features were sources of overfitting.  Drop from the model.\n\ndrop_cols = ['installation_session_clip_count', 'installation_session_act_count', 'installation_session_game_count']\nfeat = [x for x in feat if x not in drop_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Run feature selection as described at the beginning of the notebook.  The same set of paramaters are used in each iteration.\n\nsteps = [6, 7] if run_long_version else [3, 2]\n\nfeatures_dict = {}\nmodels_dict = {}\noof_vectors = {}\nscores_final = []\nparams = {'n_estimators':2000,\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'subsample': 0.75,\n        'subsample_freq': 1,\n        'learning_rate': 0.04,\n        'feature_fraction': 0.9,\n        'max_depth': 15,\n        'lambda_l1': 1,  \n        'lambda_l2': 1,\n        }\nn_samples = 12\nbaseline, _, _ = train_lgb(params, feat)\nfor i in range(steps[0]):\n    feats = feat\n    base = baseline\n    for drops in range(steps[1]):\n        score = 10\n        tracking = 0\n        while (score > base) & (tracking < n_samples):\n            tracking += 1\n            print(f'Iteration: {tracking}')\n            poss = [x for x in feats if x not in ['session_title']]\n            new_feats = random.sample(poss, k=len(feats)-int(len(feats)/10))\n            new_feats.append('session_title')\n            score, models, oof = train_lgb(params, new_feats)\n        print(f'Score: {score}\\nBase: {base}\\nNumber of Features: {len(feats)}')\n        if score < base:\n            base = score\n            feats = new_feats\n            features_dict[f'{i}_iteration_feat'] = feats\n            models_dict[f'{i}_iteration_models'] = models\n            oof_vectors[f'{i}_iteration_oof'] = oof\n    scores_final.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the 6 best scores.\nprint(scores_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensembling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for use with sp.optimise, returns best linear coefficients.\n\ndef cappa_combinations(weights, x, y_true):\n    y_pred = np.sum(weights * x, axis=1).values\n    opt2 = sp.optimize.minimize(fun=obj_func, x0=np.array([1.22232214, 1.73925866, 2.22506454]), args=(y_pred, y_true), method='nelder-mead')\n    score = opt2.fun\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Score each subset of three predictions on the reduce_test_tr set.\n\nimport itertools\nblend_scores = {}\nt = pd.concat([pd.Series(x) for x in oof_vectors.values()], axis=1)\nls = [i for i in range(len(oof_vectors))]\nfor subset in itertools.combinations(ls, 3):\n    subset = list(subset)\n    opt2 = sp.optimize.minimize(fun=cappa_combinations, x0=np.array([0.33, 0.33, 0.33]), args=(t[subset], y_t), method='nelder-mead')\n    score = opt2.fun\n    print(score)\n    blend_score = score\n    weights = opt2.x\n    print(opt2.x)\n    y_pred = np.sum(weights * t[subset], axis=1).values\n    opt2 = sp.optimize.minimize(fun=obj_func, x0=np.array([1.22232214, 1.73925866, 2.22506454]), args=(y_pred, y_t), method='nelder-mead')\n    coefs = opt2.x\n    blend_scores[f'{blend_score}'] = [subset, weights, coefs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the minimum score, with the index of each model used.\n\ncomb = blend_scores[min(blend_scores)]\nprint(comb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ensembled test predictions.\n\nn_fold = 5\ntest_preds = pd.DataFrame()\nfor i in comb[0]:\n    models = models_dict[f'{i}_iteration_models']\n    feats = features_dict[f'{i}_iteration_feat']\n    pred = sum([model.predict(reduce_test[feats], num_iteration=model.best_iteration) for model in models])/n_fold\n    test_preds[f'{i}_preds'] = pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission.  Final graph showing distribution of class values.\n\ny_pred = np.sum(comb[1] * test_preds.values, axis=1)\ny_pred = use_coefs(y_pred, comb[2], train_preds=False)\nsample_submission = pd.read_csv(root + 'sample_submission.csv')\nsample_submission['accuracy_group'] = y_pred.astype('int')\nsample_submission.to_csv('submission.csv', index=False)\nprint(sample_submission['accuracy_group'].value_counts())\nprint(sample_submission)\nsample_submission['accuracy_group'].hist(figsize=(12, 8))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}